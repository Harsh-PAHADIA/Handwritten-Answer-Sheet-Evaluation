{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekted\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ekted\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import nltk\n",
    "\n",
    "import pymysql\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import requests\n",
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "import fuzzywuzzy.fuzz\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get student USN from command line argument\n",
    "student_usn = sys.argv[1]\n",
    "\n",
    "# Connect to the MySQL database\n",
    "db = pymysql.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"\",\n",
    "    database=\"aep\"\n",
    ")\n",
    "cursor = db.cursor()\n",
    "\n",
    "\n",
    "# Fetch student's answer records\n",
    "cursor.execute(\"SELECT * FROM answer WHERE usn=%s\", student_usn)\n",
    "student_data = cursor.fetchall()\n",
    "\n",
    "\n",
    "# Function to extract text from an image using OCR\n",
    "def get_text_from_image(file_to_ocr):\n",
    "    im = Image.open(file_to_ocr)\n",
    "    txt = pytesseract.image_to_string(im)\n",
    "    return txt\n",
    "\n",
    "\n",
    "# Path to the directory containing student's answer images\n",
    "directory = os.path.join(\"MachineLearning\\answerimage\")\n",
    "image_path = (r'C:\\xampp1\\htdocs\\WebApp\\AEP')\n",
    "\n",
    "\n",
    "# Loop through the student's data to process images and extract text\n",
    "for n in range(len(student_data)):\n",
    "    path = os.path.join(directory, str(student_data[n][0]))\n",
    "    image_path += str(student_data[n][0])\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "    for j in range(1, len(student_data[n])):\n",
    "        if len(student_data[n][j]) != 0:\n",
    "            image_files = re.split(r' *[\\,\\?!][\\\"\"\\)\\]]* *', student_data[n][j])\n",
    "            for img_file in image_files[:-1]:\n",
    "                img_filename = img_file.split('/')[1].split('.')[0][0] + \".txt\"\n",
    "                txt_path = os.path.join(path, img_filename)\n",
    "                if os.path.isfile(txt_path):\n",
    "                    os.remove(txt_path)\n",
    "            \n",
    "            for img_file in image_files[:-1]:\n",
    "                txt = get_text_from_image(os.path.join(image_path, img_file.split('/')[1]))\n",
    "                img_filename = img_file.split('/')[1].split('.')[0][0] + \".txt\"\n",
    "                with open(os.path.join(path, img_filename), 'a') as f:\n",
    "                    f.write(txt)\n",
    "\n",
    "\n",
    "# Function to assign marks based on the length of the answer\n",
    "def ans_len(num_words, ans_length):\n",
    "    marks = 0\n",
    "    if num_words > 250:\n",
    "        marks = 10\n",
    "    elif 230 < num_words <= 250:\n",
    "        marks = 9\n",
    "    elif 200 < num_words <= 230:\n",
    "        marks = 8\n",
    "    elif 180 < num_words <= 200:\n",
    "        marks = 7\n",
    "    elif 160 < num_words <= 180:\n",
    "        marks = 6\n",
    "    elif 140 < num_words <= 160:\n",
    "        marks = 5\n",
    "    elif 110 < num_words <= 140:\n",
    "        marks = 4\n",
    "    elif 80 < num_words <= 110:\n",
    "        marks = 3\n",
    "    elif 40 < num_words <= 80:\n",
    "        marks = 2\n",
    "    elif 3 < num_words <= 40:\n",
    "        marks = 1\n",
    "    \n",
    "    if ans_length >= 15:\n",
    "        length_marks = 5\n",
    "    elif 13 <= ans_length < 15:\n",
    "        length_marks = 4\n",
    "    elif 10 <= ans_length < 13:\n",
    "        length_marks = 3\n",
    "    elif 5 <= ans_length < 10:\n",
    "        length_marks = 2\n",
    "    elif 1 < ans_length < 5:\n",
    "        length_marks = 1\n",
    "    else:\n",
    "        length_marks = 0\n",
    "    \n",
    "    total_marks = (marks + length_marks) / 2\n",
    "    return int(total_marks)\n",
    "\n",
    "\n",
    "# Function to check grammar and spelling errors using TextGears API\n",
    "def grammar_check(sentence, num_words, ans_length):\n",
    "    api_key = \"your_textgears_api_key\"\n",
    "    req = requests.get(f\"https://api.textgears.com/check.php?text={sentence}&key={api_key}\")\n",
    "    errors = req.json()['errors']\n",
    "    count_spelling = sum(1 for error in errors if error['type'] == 'spelling')\n",
    "    count_grammar = sum(1 for error in errors if error['type'] == 'grammar')\n",
    "\n",
    "    error_rate1 = count_grammar / ans_length\n",
    "    error_rate2 = count_spelling / num_words\n",
    "    percentage_of_error = int(math.sqrt(error_rate1) + math.sqrt(error_rate2) * 10)\n",
    "    return percentage_of_error\n",
    "\n",
    "\n",
    "# Function to preprocess the sentence (remove punctuation, stop words, etc.)\n",
    "import string\n",
    "\n",
    "\n",
    "def preprocess_sentence(stmt):\n",
    "    stmt = stmt.lower()\n",
    "    remove = dict.fromkeys(map(ord, '\\n' + string.punctuation), \" \")\n",
    "    stmt = stmt.translate(remove)\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(stmt)\n",
    "    words = [w for w in word_tokens if w not in stop_words]\n",
    "\n",
    "    from nltk import pos_tag\n",
    "    \n",
    "    processed_sentence = \" \".join([word for word in words if pos_tag([word])[0][1] != 'IN'])\n",
    "    return processed_sentence\n",
    "\n",
    "\n",
    "\n",
    "# Function to match student's answer with the key answer\n",
    "def match_answers_with_key(Sans1, Kans1):\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "\n",
    "    answer = \"\\n\".join(preprocess_sentence(stmt) for stmt in sent_tokenize(Sans1))\n",
    "    key = \"\\n\".join(preprocess_sentence(stmt) for stmt in sent_tokenize(Kans1))\n",
    "\n",
    "    def synonym_exists(word, answer_tokens):\n",
    "        synonyms = {lemma.name() for syn in wordnet.synsets(word) for lemma in syn.lemmas()}\n",
    "        return any(synonym in answer_tokens for synonym in synonyms)\n",
    "    \n",
    "    answer_tokens = set(word_tokenize(answer))\n",
    "    key_sentences = key.split('\\n')\n",
    "\n",
    "    key_match_ratio = max(\n",
    "        sum(1 for sentence in key_sentences if sentence in answer) / len(key_sentences) * 100,\n",
    "        sum(1 for word in set(word_tokenize(key)) if word in answer_tokens or synonym_exists(word, answer_tokens)) / len(set(word_tokenize(key))) * 100\n",
    "    )\n",
    "\n",
    "    return key_match_ratio\n",
    "\n",
    "# Additional functions and evaluation logic would follow the same pattern.\n",
    "\n",
    "# Train Part\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def train_model(answer_lengths, error_rates, key_match_ratios, labels):\n",
    "    # Define the model architecture\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Train the model\n",
    "    X = np.array([answer_lengths, error_rates, key_match_ratios]).T\n",
    "    y = np.array(labels)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Evaluate the model's performance\n",
    "    y_pred = model.predict(X)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y, y_pred))\n",
    "\n",
    "# Example usage\n",
    "answer_lengths = [...]  # list of answer lengths\n",
    "error_rates = [...]  # list of error rates\n",
    "key_match_ratios = [...]  # list of key match ratios\n",
    "labels = [...]  # list of labels (e.g., 0 for incorrect, 1 for correct)\n",
    "train_model(answer_lengths, error_rates, key_match_ratios, labels)\n",
    "\n",
    "# Test Part\n",
    "\n",
    "def process_student_answer(student_data, image_path):\n",
    "    # Extract text from images using OCR\n",
    "    texts = []\n",
    "    for img_file in student_data:\n",
    "        txt = get_text_from_image(os.path.join(image_path, img_file.split('/')[1]))\n",
    "        texts.append(txt)\n",
    "    \n",
    "    # Preprocess the extracted text\n",
    "    preprocessed_texts = [preprocess_sentence(txt) for txt in texts]\n",
    "    \n",
    "    # Calculate the length of the answer\n",
    "    answer_lengths = [len(txt.split()) for txt in preprocessed_texts]\n",
    "    \n",
    "    # Check grammar and spelling errors using the TextGears API\n",
    "    error_rates = [grammar_check(txt, len(txt.split()), len(txt.split())) for txt in preprocessed_texts]\n",
    "    \n",
    "    # Match the student's answer with the key answer\n",
    "    key_match_ratios = [match_answers_with_key(txt, key_answer) for txt in preprocessed_texts]\n",
    "    \n",
    "    return answer_lengths, error_rates, key_match_ratios\n",
    "\n",
    "# Example usage\n",
    "student_data = [...]  # list of image files\n",
    "image_path = 'C:\\xampp1\\htdocs\\WebApp\\AEP'\n",
    "answer_lengths, error_rates, key_match_ratios = process_student_answer(student_data, image_path)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
